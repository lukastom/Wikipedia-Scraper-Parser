# Wikipedia-Scraper-Parser

I made this code in 2011 to experiment with a search engine indexing and SEO mechanisms and their evolution over time. This code was unzipping the whole Wikipedia in xml.bz2 format on the fly. Because the unzipping worked similar to streaming, the server was not overloaded by sudden attempt to unzip 50 GB file. Using MediaWiki 1.16.0 and bunch of my custom code (regex used richly), the articles were parsed, translated and published as posts on a Wordpress site. This helped me dummy fill the website and analyze behavior of a search engine site. I made my SEO decisions based on analysis of this experimental project and got several sites to global no. 1 in Google.
